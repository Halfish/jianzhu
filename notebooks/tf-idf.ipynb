{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用了 tf-idf 来编码文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19207\n",
      "[u'website', u'title', u'url', u'poster', u'detail', u'keywords', u'sentence_id', u'images']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "data_pair = [json.loads(data) for data in open('static/dataset/data_pair.json', 'r')]\n",
    "print len(data_pair)\n",
    "print data_pair[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache ./jieba.cache\n",
      "Loading model cost 0.193 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have 23640 words before shrink\n",
      "have 2721 words after shrink\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import jieba\n",
    "jieba.dt.tmp_dir = './'\n",
    "jieba.initialize()\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.index2word = []\n",
    "        self.word2index = {}\n",
    "        self.wordcount = {}\n",
    "        self.stopwords = [word.strip().decode('utf-8') for word in open('static/dataset/stopwords.txt', 'r')]\n",
    "        \n",
    "    def update(self, words):\n",
    "        words = re.sub(u'[\\r\\t\\n ]', u' ', words)\n",
    "        words = list(jieba.cut_for_search(words))\n",
    "        words = filter(lambda x: x not in self.stopwords, words)\n",
    "        for word in words:\n",
    "            if self.word2index.has_key(word):\n",
    "                self.wordcount[word] += 1\n",
    "            else:\n",
    "                self.wordcount[word] = 1\n",
    "                self.index2word.append(word)\n",
    "                self.word2index[word] = len(self.index2word) - 1\n",
    "                \n",
    "    def shrink(self, min_count=2):\n",
    "        index2word, word2index, wordcount = [], {}, {}\n",
    "        for word in self.index2word:\n",
    "            if self.wordcount[word] >= min_count:\n",
    "                index2word.append(word)\n",
    "                word2index[word] = len(index2word) - 1\n",
    "                wordcount[word] = self.wordcount[word]\n",
    "        self.index2word = index2word\n",
    "        self.word2index = word2index\n",
    "        self.wordcount = wordcount\n",
    "\n",
    "dic = Dictionary()             \n",
    "for data in data_pair:\n",
    "    dic.update(data['title'])\n",
    "print 'have %d words before shrink' %(len(dic.index2word))\n",
    "dic.shrink(min_count=5)\n",
    "print 'have %d words after shrink' %(len(dic.index2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19207, 2721)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tfidf(object):\n",
    "    def __init__(self, dic):\n",
    "        self.dic = dic\n",
    "        self.term_freq = []\n",
    "        self.inverse_doc_freq = {key:0 for key, value in dic.wordcount.iteritems()}\n",
    "        self.n_docs = 0\n",
    "    \n",
    "    def _clean(self, words):\n",
    "        words = re.sub('[\\r\\t\\n ]', ' ', words)\n",
    "        words = list(jieba.cut_for_search(words))\n",
    "        words = filter(lambda x: (x not in self.dic.stopwords) and (x in self.dic.wordcount), words)\n",
    "        return words\n",
    "    \n",
    "    def update(self, words):\n",
    "        words = self._clean(words)\n",
    "        #assert len(words) > 0, 'detect useless doc'\n",
    "        doc_tf = {word:0 for word in words}\n",
    "        for word in words:\n",
    "            doc_tf[word] += 1\n",
    "        self.term_freq.append(doc_tf)\n",
    "        for term in doc_tf:\n",
    "            self.inverse_doc_freq[term] += 1\n",
    "        self.n_docs += 1\n",
    "    \n",
    "    def parse(self, words):\n",
    "        words = self._clean(words)\n",
    "        doc_tf = {word:0 for word in words}\n",
    "        for word in words:\n",
    "            doc_tf[word] += 1\n",
    "        return self._get_tfidf(doc_tf)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        assert index < self.n_docs, 'out of range'\n",
    "        return self._get_tfidf(self.term_freq[index])\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i in range(self.n_docs):\n",
    "            yield self[i]\n",
    "        \n",
    "    def _get_tfidf(self, tf):\n",
    "        tfidf = {}\n",
    "        for word, freq in tf.iteritems():\n",
    "            tfidf[word] = freq * np.log(self.n_docs / (1e-6 + self.inverse_doc_freq[word]))\n",
    "        tfidf = sorted(tfidf.iteritems(), key=lambda x: x[1], reverse=True)\n",
    "        tfidf = {k:v for k, v in tfidf} # list to dict\n",
    "        return tfidf\n",
    "    \n",
    "    def to_numpy(self, tfidf):\n",
    "        doc = np.zeros(len(self.dic.wordcount))\n",
    "        for word, score in tfidf.iteritems():\n",
    "            doc[self.dic.word2index[word]] = score\n",
    "        return doc\n",
    "    \n",
    "    def numpy(self):\n",
    "        return np.array([self.to_numpy(tfidf) for tfidf in self])\n",
    "    \n",
    "tfidf = Tfidf(dic)\n",
    "for data in data_pair:\n",
    "    tfidf.update(data['title'])\n",
    "    \n",
    "docs = tfidf.numpy()\n",
    "print docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19207, 1)\n"
     ]
    }
   ],
   "source": [
    "norm = (docs ** 2).sum(1, keepdims=True)\n",
    "print norm.shape\n",
    "\n",
    "normalized_docs = docs / (norm + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('docs_tfidf', normalized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2721,)\n",
      "[  704 14624  6770  5170 15806 18794  7868 15517 10205  1232]\n",
      "[ 0.99999991  0.99999991  0.99999991  0.99999991  0.99999991  0.99999991\n",
      "  0.99999991  0.99999991  0.99999991  0.99999991]\n"
     ]
    }
   ],
   "source": [
    "query = u'酒店'\n",
    "query_tfidf = tfidf.to_numpy(tfidf.parse(query))\n",
    "print query_tfidf.shape\n",
    "\n",
    "scores = np.dot(normalized_docs, query_tfidf)\n",
    "index = scores.argsort()[::-1]\n",
    "scores = scores[index]\n",
    "print index[0:10]\n",
    "print scores[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704 皮克林宾乐雅酒店\n",
      "14624 酒店\n",
      "6770 POD酒店\n",
      "5170 Floreasca酒店\n",
      "15806 Hyderbad帕克酒店\n",
      "18794 帕西托尼酒店\n",
      "7868 酒店\n",
      "15517 艾尔酒店\n",
      "10205 寒舍艾丽酒店\n",
      "1232 坚果酒店\n"
     ]
    }
   ],
   "source": [
    "for i in index[0:10]:\n",
    "    print i, data_pair[i]['title']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
